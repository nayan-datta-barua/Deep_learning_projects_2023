{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "mount_file_id": "1O075CzcBadNzhtkNlaRLTiUGej-5XgRJ",
      "authorship_tag": "ABX9TyPguLIowLBXedoNxJ87q3w9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nayan-datta-barua/Deep_learning_projects_2023/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''i have a spark cluster connected to elasticsearch that receives data in real time from a data base\n",
        "now i need to create a mchine learning model that cana analyze data in real time and detect anomaly using pysapark\n",
        "the data contains logs from a cpu. timestamp, nodeid(identifies the cpu) and the load value i need to detect the anomaly (when there s problems on the cpu and\n",
        "his load is more than 85 and i need to predict this for next 7 days) and a timestamp column.\n",
        "the dataset has more than 3 millions logs and more than 1300nodes   \n",
        "my problem is to analyze each node and i cannot create model ml pyspark for each node cuz it s lot\n",
        "it s where m stuck'''"
      ],
      "metadata": {
        "id": "O_--URrbW1eP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDrnGRRh_ddP"
      },
      "outputs": [],
      "source": [
        "# !apt update -q\n",
        "# !apt-get install -q openjdk-11-jdk-headless\n",
        "# !curl -L https://github.com/SpencerPark/IJava/releases/download/v1.3.0/ijava-1.3.0.zip -o ijava-kernel.zip\n",
        "# !unzip -q ijava-kernel.zip -d ijava-kernel && cd ijava-kernel && python3 install.py --sys-prefix\n",
        "# !jupyter kernelspec list\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ml2PRZNGW1g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!pip install pyarrow\n",
        "!pip install prophet"
      ],
      "metadata": {
        "id": "KgUITh4ugFmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from prophet import Prophet\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import os\n",
        "from datetime import datetime\n",
        "import pyspark\n",
        "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import current_date\n",
        "import plotly.express as px\n",
        "import matplotlib as mpl\n",
        "import plotly.graph_objects as go\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import pandas_udf,PandasUDFType"
      ],
      "metadata": {
        "id": "U8bpoiTOG-S7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "metadata": {
        "id": "YnjB4s-jGQBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# = 29 \n",
        "desired_nodeid = input(\"please select which node you want to show: \" )"
      ],
      "metadata": {
        "id": "31d9eYxxFq5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"/content/my_data.csv\")"
      ],
      "metadata": {
        "id": "iJk0EIogEVBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "TulkwgfiH7xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.drop_duplicates()\n",
        "df=df.dropna()\n",
        "df['nodeid']=df['nodeid'].astype('int32')\n",
        "df['Load']=df['Load'].astype('int32')\n",
        "df['timestamp']=pd.to_datetime(df['timestamp'])\n",
        "print(\"\\n Information of this date set\",df.info())\n",
        "print(df.shape)\n",
        "print(\"data columns\",df.columns.to_list())\n",
        "print(\"Null data:\",df.isnull().any())\n",
        "print(\"Unique data:\",df.nunique)"
      ],
      "metadata": {
        "id": "8N3R-tQGIrvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df=df.iloc[:200000,:]"
      ],
      "metadata": {
        "id": "b65Lq0X_pbAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['nodeid'].value_counts().count"
      ],
      "metadata": {
        "id": "Gx56yUlBI5g3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ldf=df.set_index(\"timestamp\").resample(\"H\").mean()\n",
        "# ldf.query(\"nodeid==26\")[[\"Load\"]].plot()\n",
        "# ldf.query(\"nodeid==27\")[[\"Load\"]].plot()\n",
        "# ldf.query(\"nodeid==28\")[[\"Load\"]].plot()"
      ],
      "metadata": {
        "id": "3SSLocdAJBfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spark dateframe\n",
        "sdf=spark.createDataFrame(df)"
      ],
      "metadata": {
        "id": "vAxu4y8mK8-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sdf.show(5)"
      ],
      "metadata": {
        "id": "nGRabyF6LA2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sdf.printSchema"
      ],
      "metadata": {
        "id": "wWuVHMtMLcgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sdf.count()"
      ],
      "metadata": {
        "id": "fRIpRzv1M7-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hl8tkii5Ljpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tCGAAeMDQyLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sdf.select(['nodeid']).groupby('nodeid').agg({\"nodeid\":'count'}).show()\n",
        "sdf.createOrReplaceTempView(\"Load\")\n",
        "# spark.sql(\"select nodeid,count(*) from Load group by nodeid order by nodeid\").show()\n",
        "sql='SELECT nodeid ,timestamp as ds,sum(Load)as y FROM Load GROUP BY nodeid,ds ORDER BY nodeid,ds'\n",
        "# spark.sql(sql).show()\n",
        "sdf.rdd.getNumPartitions()\n",
        "# sdf.explain()\n",
        "load_part=(spark.sql(sql).repartition(spark.sparkContext.defaultParallelism,['nodeid'])).cache()\n",
        "# load_part\n",
        "# load_part.explain()\n",
        "\n",
        "result_schema =StructType([\n",
        "    StructField(\"ds\", TimestampType(), True),\n",
        "    StructField('nodeid',IntegerType(),True),\n",
        "    StructField('y',DoubleType()),\n",
        "    StructField('yhat',DoubleType()),\n",
        "    StructField('yhat_upper',DoubleType()),\n",
        "    StructField('yhat_lower',DoubleType()),\n",
        "\n",
        "])\n",
        "\n",
        "@pandas_udf(result_schema,PandasUDFType.GROUPED_MAP)\n",
        "def forecast_load(load_pd):\n",
        "  model =Prophet(interval_width=0.98,seasonality_mode=\"multiplicative\",weekly_seasonality = True)  #,yearly_seasonality = True\n",
        "  model.fit(load_pd)\n",
        "  \n",
        "  future_pd =model.make_future_dataframe(periods=119,freq='H')\n",
        "  forecast_pd=model.predict(future_pd)\n",
        "  f_pd=forecast_pd[ ['ds','yhat','yhat_upper','yhat_lower']].set_index('ds')\n",
        "  st_pd=load_pd[[\"ds\",\"nodeid\",'y']].set_index('ds')\n",
        "  result_pd=f_pd.join(st_pd,how='left')\n",
        "  result_pd.reset_index(level=0,inplace=True)\n",
        "  result_pd['nodeid']=load_pd['nodeid'].iloc[0]\n",
        "  return result_pd[['ds','nodeid','y','yhat',\"yhat_lower\",'yhat_upper']]"
      ],
      "metadata": {
        "id": "IOdrbc9bhe7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7tBBizDGhe-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=(load_part.groupby('nodeid').apply(forecast_load).withColumn(\"traning_time\",current_date())).cache()\n",
        "df.createOrReplaceTempView('forecasted')\n",
        "# result.cache()\n",
        "# result.explain()\n",
        "# result.count()\n",
        "df = df.withColumn('yhat_local', (F.col('yhat') + F.col('yhat_lower') + F.col('yhat_upper')) / 3.0)"
      ],
      "metadata": {
        "id": "SUCBxIpfctqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# df = df.withColumn('ds', F.to_timestamp('ds'))\n",
        "\n",
        "# Specify the desired nodeid and threshold\n",
        " # Replace with your desired nodeid\n",
        "threshold = 85\n",
        "\n",
        "# Filter the DataFrame based on the desired nodeid\n",
        "filtered_df = df.filter(df.nodeid == desired_nodeid)\n",
        "\n",
        "# Create traces for y and yhat, with color change if exceeding the threshold\n",
        "y_values = filtered_df.select('y').rdd.flatMap(lambda x: x).collect()\n",
        "yhat_values = filtered_df.select('yhat').rdd.flatMap(lambda x: x).collect()\n",
        "yhat_lower_values = filtered_df.select('yhat_lower').rdd.flatMap(lambda x: x).collect()\n",
        "yhat_upper_values = filtered_df.select('yhat_upper').rdd.flatMap(lambda x: x).collect()\n",
        "yhat_local = filtered_df.select('yhat_local').rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "\n",
        "trace_y = go.Scatter(\n",
        "    x=filtered_df.select('ds').rdd.flatMap(lambda x: x).collect(),\n",
        "    y=y_values,\n",
        "    mode='markers',\n",
        "    name='Node ID: {}'.format(desired_nodeid),\n",
        "    marker=dict(\n",
        "        color=['red' if val is not None and val > threshold else 'black' for val in y_values if val is not None],\n",
        "        symbol='circle',\n",
        "        size=8\n",
        "    ),\n",
        ")\n",
        "\n",
        "trace_yhat = go.Scatter(\n",
        "    x=filtered_df.select('ds').rdd.flatMap(lambda x: x).collect(),\n",
        "    y=yhat_values,\n",
        "    mode='lines',\n",
        "    name='Node ID: {} (yhat)'.format(desired_nodeid),\n",
        "    marker=dict(\n",
        "        color=['red' if val is not None and val > threshold else 'blue' for val in yhat_values if val is not None],\n",
        "        symbol='circle-open',\n",
        "        size=6\n",
        "    ),\n",
        ")\n",
        "\n",
        "trace_yhat_lower = go.Scatter(\n",
        "    x=filtered_df.select('ds').rdd.flatMap(lambda x: x).collect(),\n",
        "    y=yhat_lower_values,\n",
        "    mode='lines',\n",
        "    name='Node ID: {} (yhat_lower)'.format(desired_nodeid),\n",
        "    marker=dict(\n",
        "        color=['red' if val is not None and val > threshold else 'green' for val in yhat_values if val is not None],\n",
        "        symbol='circle-open',\n",
        "        size=8\n",
        "    ),\n",
        ")\n",
        "\n",
        "trace_yhat_upper = go.Scatter(\n",
        "    x=filtered_df.select('ds').rdd.flatMap(lambda x: x).collect(),\n",
        "    y=yhat_upper_values,\n",
        "    mode='lines',\n",
        "    name='Node ID: {} (yhat_upper)'.format(desired_nodeid),\n",
        "    marker=dict(\n",
        "        color=['red' if val is not None and val > threshold else 'midnightblue' for val in yhat_values if val is not None],\n",
        "        symbol='circle-open',\n",
        "        size=4\n",
        "    ),\n",
        ")\n",
        "\n",
        "trace_yhat_local = go.Scatter(\n",
        "    x=filtered_df.select('ds').rdd.flatMap(lambda x: x).collect(),\n",
        "    y=yhat_local,\n",
        "    mode='lines',\n",
        "    name='Node ID: {} (yhat_local)'.format(desired_nodeid),\n",
        "    marker=dict(\n",
        "        color=['red' if val is not None and val > threshold else 'rgb(120, 0, 223)' for val in yhat_values if val is not None],\n",
        "        symbol='circle-open',\n",
        "        size=8\n",
        "    ),\n",
        ")\n",
        "\n",
        "\n",
        "# Create the figure and add the traces\n",
        "fig = go.Figure(data=[trace_y, trace_yhat,trace_yhat_lower,trace_yhat_upper,trace_yhat_local])\n",
        "\n",
        "# Set plot layout\n",
        "fig.update_layout(\n",
        "    title=\"Time Series Data (Node ID: {})\".format(desired_nodeid),\n",
        "    xaxis_title=\"Date\",\n",
        "    yaxis_title=\"Value\"\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "lcCGW6lkM6zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df.show()"
      ],
      "metadata": {
        "id": "gPfuZtEjctm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import plotly.graph_objects as go\n",
        "# import pyspark.sql.functions as F\n",
        "\n",
        "\n",
        "\n",
        "# df = df.withColumn('ds', F.to_timestamp('ds'))\n",
        "\n",
        "# node_ids = df.select(\"nodeid\").distinct().rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "# # Create traces for each node ID\n",
        "# traces = []\n",
        "# colors = ['blue', 'red', 'green', 'orange']  # Add more colors if needed\n",
        "# for i, node_id in enumerate(node_ids):\n",
        "#     filtered_df = df.filter(df.nodeid == node_id)\n",
        "    \n",
        "#     trace_y = go.Scatter(\n",
        "#         x=filtered_df.select('ds').rdd.flatMap(lambda x: x).collect(),\n",
        "#         y=filtered_df.select('y').rdd.flatMap(lambda x: x).collect(),\n",
        "#         mode='lines',\n",
        "#         name='Node ID: {}'.format(node_id),\n",
        "#         line=dict(color=colors[i % len(colors)]),\n",
        "#     )\n",
        "    \n",
        "#     trace_yhat = go.Scatter(\n",
        "#         x=filtered_df.select('ds').rdd.flatMap(lambda x: x).collect(),\n",
        "#         y=filtered_df.select('yhat').rdd.flatMap(lambda x: x).collect(),\n",
        "#         mode='lines',\n",
        "#         name='Node ID: {} (yhat)'.format(node_id),\n",
        "#         line=dict(color=colors[i % len(colors)], dash='dash'),\n",
        "#     )\n",
        "    \n",
        "#     traces.append(trace_y)\n",
        "#     traces.append(trace_yhat)\n",
        "\n",
        "# # Create the figure and add the traces\n",
        "# fig = go.Figure(data=traces)\n",
        "\n",
        "# # Set plot layout\n",
        "# fig.update_layout(\n",
        "#     title=\"Time Series Data\",\n",
        "#     xaxis_title=\"Date\",\n",
        "#     yaxis_title=\"Value\"\n",
        "# )\n",
        "\n",
        "# # Show the plot\n",
        "# fig.show()"
      ],
      "metadata": {
        "id": "gQMs7FU0G8aT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = df.withColumn('ds', F.to_timestamp('ds'))\n",
        "# desired_nodeid = 26  # Replace with your desired nodeid\n",
        "\n",
        "# # Filter the DataFrame based on the desired nodeid\n",
        "# filtered_df = df.filter(df.nodeid == desired_nodeid)\n",
        "# threshold = 80\n",
        "# # Create traces for y and yhat\n",
        "# trace_y = go.Scatter(\n",
        "#     x=filtered_df.select('ds').rdd.flatMap(lambda x: x).collect(),\n",
        "#     y=filtered_df.select('y').rdd.flatMap(lambda x: x).collect(),\n",
        "#     mode='lines',\n",
        "#     name='Node ID: {}'.format(desired_nodeid),\n",
        "#     line=dict(color='blue'),\n",
        "#     # line=dict(color=['blue' if val <= threshold else 'red' for val in df.select('y').rdd.flatMap(lambda x: x).collect()]),\n",
        "# )\n",
        "\n",
        "# trace_yhat = go.Scatter(\n",
        "#     x=filtered_df.select('ds').rdd.flatMap(lambda x: x).collect(),\n",
        "#     y=filtered_df.select('yhat').rdd.flatMap(lambda x: x).collect(),\n",
        "#     mode='lines',\n",
        "#     name='Node ID: {} (yhat)'.format(desired_nodeid),\n",
        "#     line=dict(color='red', dash='dash'),\n",
        "#     # line=dict(color=['blue' if val <= threshold else 'red' for val in df.select('yhat').rdd.flatMap(lambda x: x).collect()], dash='dash'),\n",
        "# )\n",
        "\n",
        "# # Create the figure and add the traces\n",
        "# fig = go.Figure(data=[trace_y, trace_yhat])\n",
        "\n",
        "# # Set plot layout\n",
        "# fig.update_layout(\n",
        "#     title=\"Time Series Data (Node ID: {})\".format(desired_nodeid),\n",
        "#     xaxis_title=\"Date\",\n",
        "#     yaxis_title=\"Value\"\n",
        "# )\n",
        "\n",
        "# # Show the plot\n",
        "# fig.show()"
      ],
      "metadata": {
        "id": "oGAcF4aPIt9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# result.show()"
      ],
      "metadata": {
        "id": "HNWxuGGKxgg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final=result.toPandas()\n",
        "# final=final.set_index('ds')"
      ],
      "metadata": {
        "id": "6f_nQDAI92tW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final"
      ],
      "metadata": {
        "id": "zITRgRgBhfG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final['ylocal']=((final['yhat']+final['yhat_lower']+final['yhat_upper'])/3).abs()"
      ],
      "metadata": {
        "id": "hWZjG-3zZF-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final.iloc[:20000,:].tail()"
      ],
      "metadata": {
        "id": "5SEzAjugZGA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from prophet.plot import plot_components\n",
        "# final.query('nodeid==26')[['y','yhat','yhat_upper','yhat_lower','ylocal']].plot()\n"
      ],
      "metadata": {
        "id": "7M0daCa97XpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final"
      ],
      "metadata": {
        "id": "fIzhkYS196pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final[final['y']>80].head()"
      ],
      "metadata": {
        "id": "pQgwtMJWf-Gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fJzS2XQtVXWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "V0g_8YUJPC9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0wufgWBKWsKB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}